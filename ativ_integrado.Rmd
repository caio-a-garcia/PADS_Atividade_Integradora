---
title: "Atividade Integradora 1 Trimestre"
output: html_document
date: "2023-09-06"
---



```{r setup, include=FALSE}

library(tidyverse)
library(tidymodels)
library(dbplyr)
library(ggplot2)      #Biblioteca para plots gráficos
library(rpart)        #Biblioteca para árvore de regressão
library(rpart.plot)   
library(skimr)        #Biblioteca para overview dos dados
library(rsample)
library(ranger)       #Biblioteca para Random Forest
library(janitor)      #Biblioteca pra remoção de colunas
library(glmnet)       #Biblioteca para modelos de regressão com encolhimento (Ridge, LASSO, elastic-net)
library(plotmo)       #Plotar os gráficos dos coeficientes Ridge
library(pROC)
library(caret)        #Biblioteca para confusion matrix
library(ada)          #Biblioteca para adaBoosting

```

# Lendo os dados

```{r}
df <- read.csv("data_2012.csv")

head(skim(df))
head(summary(df))
```


# Vamos considerar os seguintes modelos:

1.  Logistic regresion
2.  shrinkage methods
3.  Random Forest
4.  Boosting methods

# Treinamento x Teste

```{r}
df <- df %>% 

  mutate(fechado = factor(fechado, levels = c(0, 1), labels = c("open", "closed")))
```

```{r}
table(df$fechado)
```

```{r}
set.seed(123)

splits <- initial_split(df, prop = .8, strata ='fechado')

treinamento <- training(splits)
teste <- testing(splits)
```

```{r}
table(treinamento$fechado)
```

```{r}
table(teste$fechado)
```

# Tabela de comparação dos modelos de predição

```{r}
resultados <- tibble(modelo = c("Logistic", "Shrinkage", 
                                "Random Forest", "Boosting"),
                     accuracy = NA,
                     AUC = NA,
                     sensitivity = NA,
                     specificity = NA)
```

Obs.: As métricas escolhidas para avaliar o desempenho foram a acurácia geral, a AUC da curva ROC, sensibilidade (verdadeiro positivo) e specificidae (verdadeiro negativo).

No caso de nosso modelo a sensibilidade se refere a "verdadeiros abertos" e specificidade aos "verdadeiros fechados".

A acurácia é um bom parâmetro base para avaliar o erro geral do modelo.
Assim acrescentamos a AUC que nos fornece uma métrica para comparar a discriminação do modelo no geral.

Por fim analisamos individualmente os casos de verdadeiras ocorrências de ambos os casos utilizando a sensibilidade e specificidade.

Váriáveis identificadas para relevância:
```{r}

prediction_vars <-c("log_sales", "fixed_assets", "tang_assets", "profit_margin", "material_exp", "inc_bef_tax", "profit_loss_year", "personnel_exp", "curr_assets")

formula <- paste("fechado ~", paste(prediction_vars, collapse = " + "))
```

# Modelos:

## Modelo 1

#### Regressão Logística

```{r}
resultados_logist <- tibble(modelo = c("primeiro", "segundo"),
                            accuracy = NA,
                            AUC = NA,
                            sensitivity = NA,
                            specificity = NA)
```


Primeira iteração:
Todas as variáveis menos comp_id

```{r}
# Fit
fit_logistic <- glm(fechado ~ ., data = treinamento, family = binomial)

# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")

# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, "closed", "open")

# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)

```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)

# Calculate AUC
auc_value <- auc(roc_obj)


# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)

# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)

sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
```

```{r}
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
```

```{r}
plot(roc_obj, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
```


Segunda iteração:
Grupo de variáveis "prediction_vars"

```{r}
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)

# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")

# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, "closed", "open")

# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)

# Calculate AUC
auc_value <- auc(roc_obj)


# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)

# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)

sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
```

Salvando resultados:

```{r}
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
```

```{r}
plot(roc_obj, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
```


### Melhores resultados do modelo Logístico:

```{r}
resultados_logist
```


```{r, include=FALSE}
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]


resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
```



# Modelo 1 parte 2 - Electric boogalo

Vamos agora refazer as iterações do modelo logístico variando o híperparametro de corte para predição na probabilidade.

Primeira Iteração:

```{r}
# Fit
fit_logistic <- glm(fechado ~ ., data = treinamento, family = binomial)

# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")

# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")

# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)

```

Calculando métricas de desempenho:

```{r, include=FALSE}
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)

# Calculate AUC
auc_value <- auc(roc_obj)


# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)

# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)

sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
```

```{r, include=FALSE}
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
```

Segunda iteração:

```{r}
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)

# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")

# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")

# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
```

Calculando métricas de desempenho:

```{r, include=FALSE}
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)

# Calculate AUC
auc_value <- auc(roc_obj)


# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)

# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)

sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
```

Salvando resultados:

```{r, include=FALSE}
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
```

### Melhores resultados do modelo Logístico:

```{r}
resultados_logist
```


```{r, include=FALSE}
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]


resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
```


## Modelo 2

#### Encolhimento - LASSO

retiramos as variáveis não numéricas do modelo para simplificar a predição,
escolhemos também o lasso para auxiliar no processo de seleção de variáveis.

```{r}
# Preparando para glmnet ------------------------------

columns_exclude <- c("gender", "origin", "region_m")
# Desconsideramos as variáveis não numéricas para o modelo

#desconsiderando variável dependente:

x <- as.matrix(treinamento[, -c(which(names(treinamento)=="fechado"), which(names(treinamento) %in% columns_exclude))])

y <- treinamento$fechado

x_test <- as.matrix(teste[, -c(which(names(teste)=="fechado"), which(names(teste) %in% columns_exclude))])
y_test <- teste$fechado

#definindo sequência de lambda
lambda_seq <- 10^seq(0, -4, length=100)
```

Primeiro fit, otimizando lambda com cross-validation:

```{r}
# LASSO com cross-validation para econtrar lambda ótimo:
fit_lasso_cv <- cv.glmnet(x, y, family = "binomial", 
                          alpha = 1, lambda = lambda_seq)

lambda_best <- fit_lasso_cv$lambda.min
```

Predição com fit do modelo e lambda otimizado:

```{r}
# Predict probabilities for the test data
depend_predict_logistic_lasso <- predict(fit_lasso_cv, newx = x_test, s = lambda_best, type = "response")

# Convert probabilities to binary predictions
depend_predict_bin_lasso <- ifelse(depend_predict_logistic_lasso > 0.5, "closed", "open")

# Calculate accuracy
accuracy_lasso <- mean(depend_predict_bin_lasso == y_test)
```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj_lasso <- roc(y_test, depend_predict_logistic_lasso)

# Calculate AUC
auc_value_lasso <- auc(roc_obj_lasso)

# Ensure that the predicted values and true labels are factors with the same levels
depend_predict_bin_lasso <- as.factor(depend_predict_bin_lasso)
y_test <- as.factor(y_test)

# Create a confusion matrix
conf_matrix_lasso <- confusionMatrix(depend_predict_bin_lasso, y_test)

# Calculate sensitivity and specificity
sensitivity_value_lasso <- conf_matrix_lasso$byClass["Sensitivity"]
specificity_value_lasso <- conf_matrix_lasso$byClass["Specificity"]
```

Salvando resultados:

```{r, include=FALSE}
# Salvando resultados:

resultados$accuracy[resultados$modelo == "Shrinkage"] <- accuracy_lasso
resultados$AUC[resultados$modelo == "Shrinkage"] <- auc_value_lasso
resultados$sensitivity[resultados$modelo == "Shrinkage"] <- sensitivity_value_lasso
resultados$specificity[resultados$modelo == "Shrinkage"] <- specificity_value_lasso
```

```{r}
plot(roc_obj_lasso, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
```

```{r}
cat("Accuracy:", accuracy_lasso, "\n")
cat("AUC:", auc_value_lasso, "\n")
cat("Sensitivity:", sensitivity_value_lasso, "\n")
cat("Specificity:", specificity_value_lasso, "\n")
```


## Modelo 3
#### Random Forest

```{r}
# Fit do modelo Random Fortest utilizando todas as variáveis
(rf <- ranger(fechado ~ ., classification = TRUE, data = treinamento))

```
O erro da previsão do treinamento é de 10.60%.

``` {r}
# matriz de confusão para as previsões no conjunto de treinamento
rf$confusion.matrix

```
A matriz de confusão é baseada nos dados OOB. As linhas da matriz indicam a categoria da resposta observada e as colunas indicam as categorias classificadas.

Avaliar hiperparâmetros
``` {r}

# Avaliar combinação de mtry e número de árvores

tibble(arvores = seq(25, 500, 25))

resultados_floresta <- crossing (mtry = c(4, 8),
                        n_arvores = c(1:15, seq(50, 500, 50)))

ajusta <- function(mtry, n_arvores) {
  
  rf <- ranger(fechado ~ ., num.trees = n_arvores, mtry = mtry, data = treinamento)
  
  return(rf$prediction.error)
}

resultados_floresta <- resultados_floresta %>% 
  mutate(erro = map2_dbl(mtry, n_arvores, ajusta))

head(resultados_floresta)

```

Vamos avaliar o menor erro obtido pelo modelo de otimização:
```{r}
resultados_floresta %>% 
  arrange(erro)
```
Conforme observado na tabela acima, a combinação que diminui o erro do modelo é com mtry = 8 e n_arvores = 450.


Este gráfico mostra como o erro de classificação OOB varia com diferentes números de árvores e para diferentes valores de mtry.

```{r}
resultados_floresta %>% 
  mutate(mtry = factor(mtry)) %>% 
  ggplot(aes(n_arvores, erro, group = mtry, color = mtry)) +
    geom_line(size = 1.2) +
    labs(x = "Números de Árvores", y = "Erro de Classificação (OOB)") + 
    theme_bw()
```

Mostrar variáveis com maior grau de importância para o modelo:

``` {r}
# Importância

rf1 <- ranger(fechado ~ .-sales, importance = "permutation", data = treinamento)
vip::vip(rf1, aesthetics = list(fill = "#FF5757"))
```

Modelo com os hiperparâmetros otimizados do modelo anterior:

```{r}

rf_prob <- ranger(fechado ~ ., mtry = 8, num.trees = 250, probability = TRUE, data = treinamento)

head(predict(rf_prob, teste, type="response")$predictions)

```

Modelo de predição com o novo fit otimizado:

```{r}
# Prever probabilidades
prob_fechado <- predict(rf_prob, teste, type = "response")$predictions[,2]

table(observado = teste$fechado,
      predito = ifelse(prob_fechado > .18, "closed", "open"))

rf_prob$prediction.error

```

Avaliando métricas de desempenho:

``` {r}
roc_obj_rf <- roc(teste$fechado, prob_fechado)

desempenho <- coords(roc_obj_rf, 0.18, ret = c("1-accuracy", "sensitivity", "specificity", "ppv", "npv"))

auc_value_rf <- auc(roc_obj_rf)
```

```{r, include=FALSE}
# Salvando resultados:
resultados$accuracy[resultados$modelo == "Random Forest"] <- (1-desempenho$`1-accuracy`)
resultados$AUC[resultados$modelo == "Random Forest"] <- auc_value_rf
resultados$sensitivity[resultados$modelo == "Random Forest"] <-desempenho$sensitivity
resultados$specificity[resultados$modelo == "Random Forest"] <- desempenho$specificity
```

```{r}
cat("Accuracy:", (1-desempenho$`1-accuracy`), "\n")
cat("AUC:", auc_value_rf, "\n")
cat("Sensitivity:", desempenho$sensitivity, "\n")
cat("Specificity:", desempenho$specificity, "\n")
```


## Modelo 4

#### Boosting:

- AdaBoosting

```{r}
# Fit an AdaBoost model
boost_model <- ada(fechado ~ ., data = treinamento)
```

Rodando modelo de predição:

```{r}
# Predict using the model on the test set
boost_pred <- predict(boost_model, newdata = teste)

# Extract predicted probabilities for class 1 from the AdaBoost model
boost_pred_prob <- predict(boost_model, newdata = teste, type = "prob")[, 2]

# Calculate accuracy
boost_accuracy <- mean(boost_pred == teste$fechado)
```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj_boost <- roc(teste$fechado, boost_pred_prob)

# Calculate AUC
auc_value_boost <- auc(roc_obj_boost)

# Create a confusion matrix
conf_matrix_boost <- confusionMatrix(boost_pred, teste$fechado)

# Calculate sensitivity and specificity
sensitivity_value_boost <- conf_matrix_boost$byClass["Sensitivity"]
specificity_value_boost <- conf_matrix_boost$byClass["Specificity"]
```

Salvando resultados:

```{r, include=FALSE}
resultados$accuracy[resultados$modelo == "Boosting"] <- boost_accuracy
resultados$AUC[resultados$modelo == "Boosting"] <- auc_value_boost
resultados$sensitivity[resultados$modelo == "Boosting"] <- sensitivity_value_boost
resultados$specificity[resultados$modelo == "Boosting"] <- specificity_value_boost
```

```{r}
plot(roc_obj_boost, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
```

```{r}
cat("Accuracy:", boost_accuracy, "\n")
cat("AUC:", auc_value_boost, "\n")
cat("Sensitivity:", sensitivity_value_boost, "\n")
cat("Specificity:", specificity_value_boost, "\n")
```


# Conferindo Resultados

```{r}
resultados
```