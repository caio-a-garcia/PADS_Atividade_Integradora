---
title: "Atividade Integradora 1 Trimestre"
output: html_document
date: "2023-09-06"
---

```{r setup, include=FALSE}

library(tidyverse)
library(dbplyr)
library(ggplot2)      #Biblioteca para plots gráficos
library(rpart)        #Biblioteca para árvore de regressão
library(rpart.plot)   
library(skimr)        #Biblioteca para overview dos dados
library(rsample)
library(ranger)       #Biblioteca para Random Forest
library(janitor)      #Biblioteca pra remoção de colunas
library(glmnet)       #Biblioteca para modelos de regressão com encolhimento (Ridge, LASSO, elastic-net)
library(plotmo)       #Plotar os gráficos dos coeficientes Ridge
library(pROC)
library(caret)        #Biblioteca para confusion matrix

```

# Lendo os dados

```{r}
df <- read.csv("data_2012.csv")

skim(df)
summary(df)

```

# visualizações iniciais:

```{r}

```

# Vamos considerar os seguintes modelos:

1.  Linear regresion
2.  shrinkage methods
3.  Random Forest
4.  Boosting methods

# Treinamento x Teste

```{r}
df <- df %>% 
  mutate(fechado = factor(fechado, levels = c(0, 1)))
```


```{r}
set.seed(123)

splits <- initial_split(df, prop = .8)

treinamento <- training(splits)
teste <- testing(splits)
```

# Tabela de comparação dos modelos de predição

```{r}
resultados <- tibble(modelo = c("Logistic", "Shrinkage", 
                                "Random Forest", "Boosting"),
                     accuracy = NA,
                     AUC = NA,
                     sensitivity = NA,
                     specificity = NA)
```

Obs.: As métricas escolhidas para avaliar o desempenho foram a acurácia geral, a AUC da curva ROC, sensibilidade (verdadeiro positivo) e specificidae (verdadeiro negativo).

(Escrever mais detalhes de escolha das métricas)

Váriáveis identificadas para relevância:
```{r}

prediction_vars <- c()

formula <- paste("fechado ~.", paste(prediction_vars, collapse = " + "))
```

# modelos:

## Modelo 1

Regressão Logística

```{r}
resultados_logist <- tibble(modelo = c("primeiro", "segundo"),
                            accuracy = NA,
                            AUC = NA,
                            sensitivity = NA,
                            specificity = NA)
```


Primeira iteração:
Todas as variáveis menos comp_id

```{r}
# Fit
fit_logistic <- glm(fechado ~ .-comp_id, data = treinamento, family = binomial)

# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")

# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, 1, 0)

# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)

```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)

# Calculate AUC
auc_value <- auc(roc_obj)


# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)

# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)

sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
```

```{r}
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
```


Segunda iteração:
Grupo de variáveis "prediction_vars"

```{r}
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)

# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")

# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, 1, 0)

# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)

# Calculate AUC
auc_value <- auc(roc_obj)


# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)

# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)

sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
```

Salvando resultados:

```{r}
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
```

### Melhores resultados do modelo Logístico:

```{r}
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]


resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
```


## Modelo 2

Encolhimento - LASSO

retiramos as variáveis não numéricas do modelo para simplificar a predição,
escolhemos também o lasso para auxiliar no processo de seleção de variáveis.

```{r}
# Preparando para glmnet ------------------------------

columns_exclude <- c("gender", "origin", "region_m")
# Desconsideramos as variáveis não numéricas para o modelo

#desconsiderando variável dependente:

x <- as.matrix(treinamento[, -c(which(names(treinamento)=="fechado"), which(names(treinamento) %in% columns_exclude))])

y <- treinamento$fechado

x_test <- as.matrix(teste[, -c(which(names(teste)=="fechado"), which(names(teste) %in% columns_exclude))])
y_test <- teste$fechado

#definindo sequência de lambda
lambda_seq <- 10^seq(10, -2, length=100)
```

Primeiro fit, otimizando lambda com cross-validation:

```{r}
# LASSO com cross-validation para econtrar lambda ótimo:
fit_lasso_cv <- cv.glmnet(x, y, family = "binomial", alpha = 1)

lambda_best <- fit_lasso_cv$lambda.min
```

Predição com fit do modelo e lambda otimizado:

```{r}
# Predict probabilities for the test data
depend_predict_logistic_lasso <- predict(fit_lasso_cv, newx = x_test, s = lambda_best, type = "response")

# Convert probabilities to binary predictions
depend_predict_bin_lasso <- ifelse(depend_predict_logistic_lasso > 0.5, 1, 0)

# Calculate accuracy
accuracy_lasso <- mean(depend_predict_bin_lasso == y_test)
```

Calculando métricas de desempenho:

```{r}
# Create a ROC curve object
roc_obj_lasso <- roc(y_test, depend_predict_logistic_lasso)

# Calculate AUC
auc_value_lasso <- auc(roc_obj_lasso)

# Ensure that the predicted values and true labels are factors with the same levels
depend_predict_bin_lasso <- as.factor(depend_predict_bin_lasso)
y_test <- as.factor(y_test)

# Create a confusion matrix
conf_matrix_lasso <- confusionMatrix(depend_predict_bin_lasso, y_test)

# Calculate sensitivity and specificity
sensitivity_value_lasso <- conf_matrix_lasso$byClass["Sensitivity"]
specificity_value_lasso <- conf_matrix_lasso$byClass["Specificity"]
```

Salvando resultados:

```{r}
# Salvando resultados:

resultados$accuracy[resultados$modelo == "Shrinkage"] <- accuracy_lasso
resultados$AUC[resultados$modelo == "Shrinkage"] <- auc_value_lasso
resultados$sensitivity[resultados$modelo == "Shrinkage"] <- sensitivity_value_lasso
resultados$specificity[resultados$modelo == "Shrinkage"] <- specificity_value_lasso
```



## Modelo 3

Random Forest

```{r}
# Fit


# Predição


# Calculo EQM
EQM <- 

# Calculo R quadrado
r2 <- 

# Salvando resultados:
resultados$EQM[resultados$modelo == "modelo"] <- EQM
resultados$R_squared[resultados$modelo == "modelo"] <- r2
```

## Modelo 4

Boosting

```{r}
# Fit


# Predição


# Calculo EQM
EQM <- 

# Calculo R quadrado
r2 <- 

# Salvando resultados:
resultados$EQM[resultados$modelo == "modelo"] <- EQM
resultados$R_squared[resultados$modelo == "modelo"] <- r2
```


# Conferindo Resultados

```{r}
resultados
```

