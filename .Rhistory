accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
resultados_logist
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]
resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
# Preparando para glmnet ------------------------------
columns_exclude <- c("gender", "origin", "region_m")
# Desconsideramos as variáveis não numéricas para o modelo
#desconsiderando variável dependente:
x <- as.matrix(treinamento[, -c(which(names(treinamento)=="fechado"), which(names(treinamento) %in% columns_exclude))])
y <- treinamento$fechado
x_test <- as.matrix(teste[, -c(which(names(teste)=="fechado"), which(names(teste) %in% columns_exclude))])
y_test <- teste$fechado
#definindo sequência de lambda
lambda_seq <- 10^seq(0, -4, length=100)
# LASSO com cross-validation para econtrar lambda ótimo:
fit_lasso_cv <- cv.glmnet(x, y, family = "binomial",
alpha = 1, lambda = lambda_seq)
lambda_best <- fit_lasso_cv$lambda.min
# Predict probabilities for the test data
depend_predict_logistic_lasso <- predict(fit_lasso_cv, newx = x_test, s = lambda_best, type = "response")
# Convert probabilities to binary predictions
depend_predict_bin_lasso <- ifelse(depend_predict_logistic_lasso > 0.5, "closed", "open")
# Calculate accuracy
accuracy_lasso <- mean(depend_predict_bin_lasso == y_test)
# Create a ROC curve object
roc_obj_lasso <- roc(y_test, depend_predict_logistic_lasso)
# Calculate AUC
auc_value_lasso <- auc(roc_obj_lasso)
# Ensure that the predicted values and true labels are factors with the same levels
depend_predict_bin_lasso <- as.factor(depend_predict_bin_lasso)
y_test <- as.factor(y_test)
# Create a confusion matrix
conf_matrix_lasso <- confusionMatrix(depend_predict_bin_lasso, y_test)
# Calculate sensitivity and specificity
sensitivity_value_lasso <- conf_matrix_lasso$byClass["Sensitivity"]
specificity_value_lasso <- conf_matrix_lasso$byClass["Specificity"]
# Salvando resultados:
resultados$accuracy[resultados$modelo == "Shrinkage"] <- accuracy_lasso
resultados$AUC[resultados$modelo == "Shrinkage"] <- auc_value_lasso
resultados$sensitivity[resultados$modelo == "Shrinkage"] <- sensitivity_value_lasso
resultados$specificity[resultados$modelo == "Shrinkage"] <- specificity_value_lasso
plot(roc_obj_lasso, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
plot(roc_obj_lasso, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
cat("Accuracy:", accuracy_lasso, "\n")
cat("AUC:", auc_value_lasso, "\n")
cat("Sensitivity:", sensitivity_value_lasso, "\n")
cat("Specificity:", specificity_value_lasso, "\n")
# Fit do modelo Random Fortest utilizando todas as variáveis
(rf <- ranger(fechado ~ .-comp_id, classification = TRUE, data = treinamento))
# Fit do modelo Random Fortest utilizando todas as variáveis
(rf <- ranger(fechado ~ ., classification = TRUE, data = treinamento))
# matriz de confusão para as previsões no conjunto de treinamento
rf$confusion.matrix
# Avaliar combinação de mtry e número de árvores
tibble(arvores = seq(25, 500, 25))
resultados_floresta <- crossing (mtry = c(4, 8),
n_arvores = c(1:15, seq(50, 500, 50)))
ajusta <- function(mtry, n_arvores) {
rf <- ranger(fechado ~ ., num.trees = n_arvores, mtry = mtry, data = treinamento)
return(rf$prediction.error)
}
resultados_floresta <- resultados_floresta %>%
mutate(erro = map2_dbl(mtry, n_arvores, ajusta))
head(resultados_floresta)
resultados_floresta %>%
arrange(erro)
resultados_floresta %>%
mutate(mtry = factor(mtry)) %>%
ggplot(aes(n_arvores, erro, group = mtry, color = mtry)) +
geom_line(size = 1.2) +
labs(x = "Números de Árvores", y = "Erro de Classificação (OOB)") +
theme_bw()
# Importância
rf1 <- ranger(fechado ~ .-sales, importance = "permutation", data = treinamento)
vip::vip(rf1, aesthetics = list(fill = "#FF5757"))
rf_prob <- ranger(fechado ~ ., mtry = 4, num.trees = 350, probability = TRUE, data = treinamento)
head(predict(rf_prob, teste, type="response")$predictions)
# Prever probabilidades
prob_fechado <- predict(rf_prob, teste, type = "response")$predictions[,2]
table(observado = teste$fechado,
predito = ifelse(prob_fechado > .18, "closed", "open"))
rf_prob$prediction.error
roc_obj_rf <- roc(teste$fechado, prob_fechado)
desempenho <- coords(roc_obj_rf, 0.18, ret = c("1-accuracy", "sensitivity", "specificity", "ppv", "npv"))
auc_value_rf <- auc(roc_obj_rf)
# Salvando resultados:
resultados$accuracy[resultados$modelo == "Random Forest"] <- (1-desempenho$`1-accuracy`)
resultados$AUC[resultados$modelo == "Random Forest"] <- auc_value_rf
resultados$sensitivity[resultados$modelo == "Random Forest"] <-desempenho$sensitivity
resultados$specificity[resultados$modelo == "Random Forest"] <- desempenho$specificity
cat("Accuracy:", (1-desempenho$`1-accuracy`), "\n")
cat("AUC:", auc_value_rf, "\n")
cat("Sensitivity:", desempenho$sensitivity, "\n")
cat("Specificity:", desempenho$specificity, "\n")
# Fit an AdaBoost model
boost_model <- ada(fechado ~ . -comp_id, data = treinamento)
# Fit an AdaBoost model
boost_model <- ada(fechado ~ ., data = treinamento)
# Predict using the model on the test set
boost_pred <- predict(boost_model, newdata = teste)
# Extract predicted probabilities for class 1 from the AdaBoost model
boost_pred_prob <- predict(boost_model, newdata = teste, type = "prob")[, 2]
# Calculate accuracy
boost_accuracy <- mean(boost_pred == teste$fechado)
# Create a ROC curve object
roc_obj_boost <- roc(teste$fechado, boost_pred_prob)
# Calculate AUC
auc_value_boost <- auc(roc_obj_boost)
# Create a confusion matrix
conf_matrix_boost <- confusionMatrix(boost_pred, teste$fechado)
# Calculate sensitivity and specificity
sensitivity_value_boost <- conf_matrix_boost$byClass["Sensitivity"]
specificity_value_boost <- conf_matrix_boost$byClass["Specificity"]
resultados$accuracy[resultados$modelo == "Boosting"] <- boost_accuracy
resultados$AUC[resultados$modelo == "Boosting"] <- auc_value_boost
resultados$sensitivity[resultados$modelo == "Boosting"] <- sensitivity_value_boost
resultados$specificity[resultados$modelo == "Boosting"] <- specificity_value_boost
plot(roc_obj_boost, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
cat("Accuracy:", boost_accuracy, "\n")
cat("AUC:", auc_value_boost, "\n")
cat("Sensitivity:", sensitivity_value_boost, "\n")
cat("Specificity:", specificity_value_boost, "\n")
resultados
library(tidyverse)
library(tidymodels)
library(dbplyr)
library(ggplot2)      #Biblioteca para plots gráficos
library(rpart)        #Biblioteca para árvore de regressão
library(rpart.plot)
library(skimr)        #Biblioteca para overview dos dados
library(rsample)
library(ranger)       #Biblioteca para Random Forest
library(janitor)      #Biblioteca pra remoção de colunas
library(glmnet)       #Biblioteca para modelos de regressão com encolhimento (Ridge, LASSO, elastic-net)
library(plotmo)       #Plotar os gráficos dos coeficientes Ridge
library(pROC)
library(caret)        #Biblioteca para confusion matrix
library(ada)          #Biblioteca para adaBoosting
library(tidyverse)
library(tidymodels)
library(dbplyr)
library(ggplot2)      #Biblioteca para plots gráficos
library(rpart)        #Biblioteca para árvore de regressão
library(rpart.plot)
library(skimr)        #Biblioteca para overview dos dados
library(rsample)
library(ranger)       #Biblioteca para Random Forest
library(janitor)      #Biblioteca pra remoção de colunas
library(glmnet)       #Biblioteca para modelos de regressão com encolhimento (Ridge, LASSO, elastic-net)
library(plotmo)       #Plotar os gráficos dos coeficientes Ridge
library(pROC)
library(caret)        #Biblioteca para confusion matrix
library(ada)          #Biblioteca para adaBoosting
df <- read.csv("data_2012.csv")
head(skim(df))
head(summary(df))
df <- df %>%
mutate(fechado = factor(fechado, levels = c(0, 1), labels = c("open", "closed")))
table(df$fechado)
set.seed(123)
splits <- initial_split(df, prop = .8, strata ='fechado')
treinamento <- training(splits)
teste <- testing(splits)
table(treinamento$fechado)
table(teste$fechado)
resultados <- tibble(modelo = c("Logistic", "Shrinkage",
"Random Forest", "Boosting"),
accuracy = NA,
AUC = NA,
sensitivity = NA,
specificity = NA)
prediction_vars <-c("log_sales", "fixed_assets", "tang_assets", "profit_margin", "material_exp", "inc_bef_tax", "profit_loss_year", "personnel_exp", "curr_assets")
formula <- paste("fechado ~", paste(prediction_vars, collapse = " + "))
resultados_logist <- tibble(modelo = c("primeiro", "segundo"),
accuracy = NA,
AUC = NA,
sensitivity = NA,
specificity = NA)
# Fit
fit_logistic <- glm(fechado ~ ., data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
plot(roc_obj, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
plot(roc_obj, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
resultados_logist
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]
resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
# Fit
fit_logistic <- glm(fechado ~ ., data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
resultados_logist
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]
resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
# Preparando para glmnet ------------------------------
columns_exclude <- c("gender", "origin", "region_m")
# Desconsideramos as variáveis não numéricas para o modelo
#desconsiderando variável dependente:
x <- as.matrix(treinamento[, -c(which(names(treinamento)=="fechado"), which(names(treinamento) %in% columns_exclude))])
y <- treinamento$fechado
x_test <- as.matrix(teste[, -c(which(names(teste)=="fechado"), which(names(teste) %in% columns_exclude))])
y_test <- teste$fechado
#definindo sequência de lambda
lambda_seq <- 10^seq(0, -4, length=100)
# LASSO com cross-validation para econtrar lambda ótimo:
fit_lasso_cv <- cv.glmnet(x, y, family = "binomial",
alpha = 1, lambda = lambda_seq)
library(tidyverse)
library(tidymodels)
library(dbplyr)
library(ggplot2)      #Biblioteca para plots gráficos
library(rpart)        #Biblioteca para árvore de regressão
library(rpart.plot)
library(skimr)        #Biblioteca para overview dos dados
library(rsample)
library(ranger)       #Biblioteca para Random Forest
library(janitor)      #Biblioteca pra remoção de colunas
library(glmnet)       #Biblioteca para modelos de regressão com encolhimento (Ridge, LASSO, elastic-net)
library(plotmo)       #Plotar os gráficos dos coeficientes Ridge
library(pROC)
library(caret)        #Biblioteca para confusion matrix
library(ada)          #Biblioteca para adaBoosting
df <- read.csv("data_2012.csv")
head(skim(df))
head(summary(df))
df <- df %>%
mutate(fechado = factor(fechado, levels = c(0, 1), labels = c("open", "closed")))
table(df$fechado)
set.seed(123)
splits <- initial_split(df, prop = .8, strata ='fechado')
treinamento <- training(splits)
teste <- testing(splits)
table(treinamento$fechado)
table(teste$fechado)
resultados <- tibble(modelo = c("Logistic", "Shrinkage",
"Random Forest", "Boosting"),
accuracy = NA,
AUC = NA,
sensitivity = NA,
specificity = NA)
prediction_vars <-c("log_sales", "fixed_assets", "tang_assets", "profit_margin", "material_exp", "inc_bef_tax", "profit_loss_year", "personnel_exp", "curr_assets")
formula <- paste("fechado ~", paste(prediction_vars, collapse = " + "))
resultados_logist <- tibble(modelo = c("primeiro", "segundo"),
accuracy = NA,
AUC = NA,
sensitivity = NA,
specificity = NA)
# Fit
fit_logistic <- glm(fechado ~ ., data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
plot(roc_obj, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.5, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
plot(roc_obj, print.auc = TRUE, grid = TRUE, legacy.axes = TRUE, xlab = "1 - Specificity", ylab = "Sensitivity")
resultados_logist
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]
resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
# Fit
fit_logistic <- glm(fechado ~ ., data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste_bin <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste_bin)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "primeiro"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "primeiro"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "primeiro"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "primeiro"] <- specificity_value
# Fit
fit_logistic <- glm(formula, data = treinamento, family = binomial)
# Predição probabilidade de ser 1
depend_predict_logistic <- predict(fit_logistic, newdata = teste, type = "response")
# Convertendo probabilidade para binário:
depend_predict_bin <- ifelse(depend_predict_logistic > 0.18, "closed", "open")
# Calculando acurácia:
accuracy <- mean(depend_predict_bin == teste$fechado)
# Create a ROC curve object
roc_obj <- roc(teste$fechado, depend_predict_logistic)
# Calculate AUC
auc_value <- auc(roc_obj)
# Calculate sensitivity and specificity
depend_predict_bin <- as.factor(depend_predict_bin)
teste$fechado <- as.factor(teste$fechado)
# Cria matrix de confusão
conf_matrix <- confusionMatrix(depend_predict_bin, teste$fechado)
sensitivity_value <- conf_matrix$byClass["Sensitivity"]
specificity_value <- conf_matrix$byClass["Specificity"]
# Salvando resultados:
resultados_logist$accuracy[resultados_logist$modelo == "segundo"] <- accuracy
resultados_logist$AUC[resultados_logist$modelo == "segundo"] <- auc_value
resultados_logist$sensitivity[resultados_logist$modelo == "segundo"] <- sensitivity_value
resultados_logist$specificity[resultados_logist$modelo == "segundo"] <- specificity_value
resultados_logist
best_row_idx <- which.max(resultados_logist$AUC)
best_model <- resultados_logist[best_row_idx, ]
resultados$accuracy[resultados$modelo == "Logistic"] <- best_model$accuracy
resultados$AUC[resultados$modelo == "Logistic"] <- best_model$AUC
resultados$sensitivity[resultados$modelo == "Logistic"] <- best_model$sensitivity
resultados$specificity[resultados$modelo == "Logistic"] <- best_model$specificity
# Preparando para glmnet ------------------------------
columns_exclude <- c("gender", "origin", "region_m")
# Desconsideramos as variáveis não numéricas para o modelo
#desconsiderando variável dependente:
x <- as.matrix(treinamento[, -c(which(names(treinamento)=="fechado"), which(names(treinamento) %in% columns_exclude))])
y <- treinamento$fechado
x_test <- as.matrix(teste[, -c(which(names(teste)=="fechado"), which(names(teste) %in% columns_exclude))])
y_test <- teste$fechado
#definindo sequência de lambda
lambda_seq <- 10^seq(0, -4, length=100)
# LASSO com cross-validation para econtrar lambda ótimo:
fit_lasso_cv <- cv.glmnet(x, y, family = "binomial",
alpha = 1, lambda = lambda_seq)
